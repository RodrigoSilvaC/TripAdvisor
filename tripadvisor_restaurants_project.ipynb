{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "*# Project: Web Scraping TripAdvisor restaurants located in Mexico."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Goal*** : The main goal of this project is to scrap the contents of tripadvisor and create an interactive map in which the user can see how many restaurants are available in the country, and get more info about the restaurant when hovering the mouse near the location of the restaurant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from geopy.geocoders import GoogleV3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Scrape the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to scrape the data. I used scrapy to perform this task. This library creates a spider that crawls the website and gets the requested info using css or xpath selectors.\n",
    "\n",
    "For TripAdvisor, we wanted to get:\n",
    "* Name of the restaurant;\n",
    "* Address;\n",
    "* Mail;\n",
    "* Phone number;\n",
    "* Rating out of 5 stars;\n",
    "* Number of reviews;\n",
    "* Price range; and\n",
    "* Type of cuisine.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to extract all possible data from Trip Advisor, we used a special type of spyder called CrawlSpider, which contains rules attributes that specify how to extract the links from a page and which callbacks should be called for those links. They are handled by the default parse() method implemented in that class.\n",
    "\n",
    "For each of the 32 states of Mexico, I used the spider file *trip_states.py*, in which I specified:\n",
    "* The pagination rule, so the spider could deal with the pagination\n",
    "* The allowed domains for the spider (<www.tripadvisor.com.mx>);\n",
    "* The starting URL in which the spider would start scraping the data. As an example, for the state of Yucatan, **[this was the starting URL that the spider used to perform the web scraping](https://www.tripadvisor.com.mx/Restaurants-g1632078-Campeche_Yucatan_Peninsula.html)**;\n",
    "* The creation of a parse function, which selects, using xpath selectors, specific sections of the website and scrapes the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After running the spider, the data was stored as a csv file, with the following columns:\n",
    "* ***entity/city***: name of the state\n",
    "* ***name***: name of the restaurant\n",
    "* ***address***: address of the restaurant\n",
    "* ***mail***: email\n",
    "* ***phone_number***: phone number\n",
    "* ***score***: score out of 5 stars\n",
    "* ***no_reviews***: number of reviews that the restaurant has gotten\n",
    "* ***ranking_subclass_1***, ***ranking_subclass_2***, and ***ranking_subclass_3***: the restaurant's cuisine specialty state ranking\n",
    "* ***ranking_city_1***,***ranking_city_2***, and ***ranking_city_3***: the overall city ranking\n",
    "* ***price_range***: the average price range of the ticket\n",
    "* ***food_type***: the cuisine specialty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the end, I ended up with 32 csv files, one for each state. ![Here is an example of the resulting csv](csv_example.PNG \"Example csv\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Cleaning the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the goal was to clean the csv files and combine them into one single csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combining the csv files into one single csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#change directory to a folder in which all csv are located\n",
    "os.chdir('D:/Users/rsilva/Documents/Python Scripts/webscrap/projects/tripadvisor_states/scraped_files/')\n",
    "\n",
    "extension = 'csv' #define file extension that glob is going to look\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))] #get file names for all csv files located in the folder.\n",
    "\n",
    "#concatenate all csv files into one single csv, dropping duplicates\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv = combined_csv.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cleaning the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To clean the data, I created a function, ***clean_df***, that does the following tasks:\n",
    "* For the *mail* column, it removes the string \"mailto:\"\n",
    "* For the phone numbers, it removes the string \"tel:\"\n",
    "* Removes special characters from ranking columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    '''\n",
    "    remove special characters and unwanted words from scraped data\n",
    "    '''\n",
    "\n",
    "    df['mail'] = df['mail'].str.replace('mailto:','').str.replace('\\?subject=\\?','')\n",
    "    df['phone_number'] = df['phone_number'].str.replace('tel:','')\n",
    "    df['ranking_subclass_1'] = df['ranking_subclass_1'].str.replace(u'N.ยบ\\xa0',u'')\n",
    "    df['ranking_subclass_2'] = df['ranking_subclass_2'].str.replace(' de ','')\n",
    "    df['ranking_city_1'] = df['ranking_city_1'].str.replace(u'N.ยบ\\xa0',u'')\n",
    "    df['ranking_city_2'] = df['ranking_city_2'].str.replace(' de ','')\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Getting the coordinates for each restaurant scraped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One crucial step to map the restaurants is to get the coordinates. One way to obtain these coordinates is to use the library ***geopy***. Geopy locates coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoders.\n",
    "\n",
    "In this project, I used the **[Google Maps V3 API](https://geopy.readthedocs.io/en/stable/#googlev3)**, which requires the user to create an API key from Google <https://developers.google.com/maps/documentation/geolocation/get-api-key>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_coordinates(df):\n",
    "    '''\n",
    "    get longitude and latitude from address using geopy and google geolocator\n",
    "    '''\n",
    "    geolocator = GoogleV3(api_key='INSERT VALID API HERE') #must insert valid google API\n",
    "\n",
    "    latitudes=[]\n",
    "    longitudes=[]\n",
    "    addresses = df['address'].tolist()\n",
    "    for address in addresses:\n",
    "        try:\n",
    "\n",
    "            location = geolocator.geocode(address)\n",
    "            if location is not None:\n",
    "                latitudes.append(location.latitude)\n",
    "                longitudes.append(location.longitude)\n",
    "            else:\n",
    "                print(f\"Could not find Location for {address!r}\")\n",
    "                latitudes.append('NA')\n",
    "                longitudes.append('NA')\n",
    "        except GeocoderUnavailable as e:\n",
    "            latitudes.append('NA')\n",
    "            longitudes.append('NA')\n",
    "\n",
    "    df = df.assign(lat = latitudes)\n",
    "    df = df.assign(lon = longitudes)\n",
    "\n",
    "    return df\n",
    "\n",
    "#export to csv\n",
    "df.to_csv('tripadvisor_restaurants.csv', index = False, encoding='utf-8-sig')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Map the data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final step is to create the map. I used the **[Folium MarkerCluster plugin](https://python-visualization.github.io/folium/plugins.html#folium.plugins.MarkerCluster)**, which enables us to cluster several markers together and make the map more visually appealing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Users/rsilva/Documents/Python Scripts/webscrap/projects/tripadvisor/tripadvisor_restaurants_final.csv', encoding = 'utf-8-sig')\n",
    "df['name'] = df['name'].replace(r'[^\\w\\s]|_', '', regex=True)\n",
    "\n",
    "df['name'] = df['name'].astype(pd.StringDtype())\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df.count())\n",
    "#create map object with centering at coordinates mean\n",
    "m = folium.Map(location=df[[\"lat\", \"lon\"]].mean().to_list(), zoom_start=5)\n",
    "\n",
    "#filter so restaurants shown are in Mexico only\n",
    "df = df[df['lon'].between(-118.5, -86.3)]\n",
    "df = df[df['lat'].between(14.2, 33.3)]\n",
    "\n",
    "\n",
    "#create popup function to customize popup\n",
    "def popup_html(row):\n",
    "    i = row\n",
    "    restaurant_name = df['name'].iloc[i]\n",
    "    restaurant_mail = df['mail'].iloc[i]\n",
    "    restaurant_phone = df['phone_number'].iloc[i]\n",
    "    reviews = df['no_reviews'].iloc[i]\n",
    "    score = df['score'].iloc[i]\n",
    "    food_types = df['food_type'].iloc[i]\n",
    "\n",
    "    left_col_color = \"#0063B2FF\"\n",
    "    right_col_color = \"#9CC3D5FF\"\n",
    "\n",
    "    html = \"\"\"<!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <h4 style=\"margin-bottom:10\"; width=\"200px\">{}</h4>\"\"\".format(restaurant_name) + \"\"\"\n",
    "    </head>\n",
    "        <table style=\"height: 126px; width: 350px;\">\n",
    "    <tbody>\n",
    "    <tr>\n",
    "    <td style=\"background-color: \"\"\" + left_col_color + \"\"\";\"><span style=\"color: #ffffff;\">Phone Number</span></td>\n",
    "    <td style=\"width: 150px;background-color: \"\"\" + right_col_color + \"\"\";\">{}</td>\"\"\".format(restaurant_phone) + \"\"\"\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td style=\"background-color: \"\"\" + left_col_color + \"\"\";\"><span style=\"color: #ffffff;\">Contact E-mail</span></td>\n",
    "    <td style=\"width: 150px;background-color: \"\"\" + right_col_color + \"\"\";\">{}</td>\"\"\".format(restaurant_mail) + \"\"\"\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td style=\"background-color: \"\"\" + left_col_color + \"\"\";\"><span style=\"color: #ffffff;\">Number of reviews</span></td>\n",
    "    <td style=\"width: 150px;background-color: \"\"\" + right_col_color + \"\"\";\">{}</td>\"\"\".format(reviews) + \"\"\"\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td style=\"background-color: \"\"\" + left_col_color + \"\"\";\"><span style=\"color: #ffffff;\">Score</span></td>\n",
    "    <td style=\"width: 150px;background-color: \"\"\" + right_col_color + \"\"\";\">{}</td>\"\"\".format(score) + \"\"\"\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td style=\"background-color: \"\"\" + left_col_color + \"\"\";\"><span style=\"color: #ffffff;\">Type of Food</span></td>\n",
    "    <td style=\"width: 150px;background-color: \"\"\" + right_col_color + \"\"\";\">{}</td>\"\"\".format(food_types) + \"\"\"\n",
    "    </tr>\n",
    "    </tbody>\n",
    "    </table>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    location = (df[\"lat\"].iloc[i], df[\"lon\"].iloc[i])\n",
    "    html = popup_html(i)\n",
    "    iframe = branca.element.IFrame(html = html, width=510, height=280)\n",
    "    # my_string = 'name: {}\\n, phone: {}\\n, food_type: {}'.format(r['name'], r['phone_number'], r['food_type'])\n",
    "    popup = folium.Popup(folium.Html(html, script=True), max_width=500)\n",
    "    folium.Marker(location=location, popup = popup).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m.save('tripadvisor_restaurants_mexico2.html')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that's a wrap! [Here's the resulting TripAdvisor map](tripadvisor_restaurants_mexico2.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Here is an example of the map 1](map01.PNG \"Map\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Here is an example of the map](map02.PNG \"Map zoomed in\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
